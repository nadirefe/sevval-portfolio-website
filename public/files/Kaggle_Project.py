# -*- coding: utf-8 -*-
"""Kaggle_Project_fin

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z8G5Ojb4mR4lBjItAO5Fxc38rkdxFy29
"""

import pandas as pd
import numpy as np

train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')
print(train.shape)
print(test.shape)
train.tail()

train['WorkProductionFrequency'].unique()

train_label = train.pop('JobSatisfaction')
print(train_label.shape)
train_label.head()

train=train.drop('MLToolNextYearSelect', axis = 1)
train=train.drop('MLMethodNextYearSelect', axis = 1)
train=train.drop('PastJobTitlesSelect', axis = 1)
train=train.drop('MLSkillsSelect', axis = 1)
train=train.drop('MLTechniquesSelect', axis = 1)
train=train.drop('WorkInternalVsExternalTools', axis = 1)


test=test.drop('MLToolNextYearSelect', axis = 1)
test=test.drop('MLMethodNextYearSelect', axis = 1)
test=test.drop('PastJobTitlesSelect', axis = 1)
test=test.drop('MLSkillsSelect', axis = 1)
test=test.drop('MLTechniquesSelect', axis = 1)
test=test.drop('WorkInternalVsExternalTools', axis = 1)

del train['ID']
del test['ID']

#Fill the NaNs in numerical columns with the mean

num_cols2 = train.select_dtypes(include=['float','int']).columns

for columns in num_cols2:
    train[columns].fillna(train[columns].mean(), inplace=True)
    test[columns].fillna(test[columns].mean(), inplace=True)

train['WorkProductionFrequency'] = train['WorkProductionFrequency'].replace(np.nan, "Don't know")
train['LearningPlatformUsefulnessBlogs'] = train['LearningPlatformUsefulnessBlogs'].replace(np.nan, 'Not Useful')
train['LearningPlatformUsefulnessKaggle'] = train['LearningPlatformUsefulnessKaggle'].replace(np.nan, 'Not Useful')
train['LearningPlatformUsefulnessCourses'] = train['LearningPlatformUsefulnessCourses'].replace(np.nan, 'Not Useful')
train['LearningPlatformUsefulnessProjects'] = train['LearningPlatformUsefulnessProjects'].replace(np.nan, 'Not Useful')
train['LearningPlatformUsefulnessSO'] = train['LearningPlatformUsefulnessSO'].replace(np.nan, 'Not Useful')
train['LearningPlatformUsefulnessTextbook'] = train['LearningPlatformUsefulnessTextbook'].replace(np.nan, 'Not Useful')
train['LearningPlatformUsefulnessYouTube'] = train['LearningPlatformUsefulnessYouTube'].replace(np.nan, 'Not Useful')

train['WorkProductionFrequency'] = train['WorkProductionFrequency'].replace(np.nan, "Don't know")
train['WorkToolsFrequencyR'] = train['WorkToolsFrequencyR'].replace(np.nan, 'Rarely')
train['WorkToolsFrequencySQL'] = train['WorkToolsFrequencySQL'].replace(np.nan, 'Rarely')
train['WorkToolsFrequencyPython'] = train['WorkToolsFrequencyPython'].replace(np.nan, 'Rarely')
train['WorkMethodsFrequencyCross-Validation'] = train['WorkMethodsFrequencyCross-Validation'].replace(np.nan, 'Rarely')
train['WorkMethodsFrequencyDataVisualization'] = train['WorkMethodsFrequencyDataVisualization'].replace(np.nan, 'Rarely')
train['WorkMethodsFrequencyDecisionTrees'] = train['WorkMethodsFrequencyDecisionTrees'].replace(np.nan, 'Rarely')
train['WorkMethodsFrequencyLogisticRegression'] = train['WorkMethodsFrequencyLogisticRegression'].replace(np.nan, 'Rarely')

train['WorkMethodsFrequencyNeuralNetworks'] = train['WorkMethodsFrequencyNeuralNetworks'].replace(np.nan, 'Rarely')
train['WorkMethodsFrequencyPCA'] = train['WorkMethodsFrequencyPCA'].replace(np.nan, 'Rarely')
train['WorkMethodsFrequencyRandomForests'] = train['WorkMethodsFrequencyRandomForests'].replace(np.nan, 'Rarely')
train['WorkMethodsFrequencyTimeSeriesAnalysis'] = train['WorkMethodsFrequencyTimeSeriesAnalysis'].replace(np.nan, 'Rarely')
train['WorkChallengeFrequencyPolitics'] = train['WorkChallengeFrequencyPolitics'].replace(np.nan, 'Rarely')
train['WorkChallengeFrequencyUnusedResults'] = train['WorkChallengeFrequencyUnusedResults'].replace(np.nan, 'Rarely')
train['WorkChallengeFrequencyDirtyData'] = train['WorkChallengeFrequencyDirtyData'].replace(np.nan, 'Rarely')

train['WorkChallengeFrequencyExplaining'] = train['WorkChallengeFrequencyExplaining'].replace(np.nan, 'Rarely')
train['WorkChallengeFrequencyTalent'] = train['WorkChallengeFrequencyTalent'].replace(np.nan, 'Rarely')
train['WorkChallengeFrequencyClarity'] = train['WorkChallengeFrequencyClarity'].replace(np.nan, 'Rarely')
train['WorkChallengeFrequencyDataAccess'] = train['WorkChallengeFrequencyDataAccess'].replace(np.nan, 'Rarely')

train['DataScienceIdentitySelect'] = train['DataScienceIdentitySelect'].replace(np.nan, 'No')
train['TitleFit'] = train['TitleFit'].replace(np.nan, 'Poorly')
train['GenderSelect'] = train['GenderSelect'].replace(np.nan, 'A different identity')
train['Country'] = train['Country'].replace(np.nan, 'Other')
train['LanguageRecommendationSelect'] = train['LanguageRecommendationSelect'].replace(np.nan, 'Other')
train['FormalEducation'] = train['FormalEducation'].replace(np.nan, 'I prefer not to answer')
train['MajorSelect'] = train['MajorSelect'].replace(np.nan, 'Other')
train['Tenure'] = train['Tenure'].replace(np.nan, "I don't write code to analyze data")
train['EmployerIndustry'] = train['EmployerIndustry'].replace(np.nan, 'Other')
train['EmployerSize'] = train['EmployerSize'].replace(np.nan, 'I prefer not to answer')
train['WorkAlgorithmsSelect'] = train['WorkAlgorithmsSelect'].replace(np.nan, 'Other')
train['WorkDataVisualizations'] = train['WorkDataVisualizations'].replace(np.nan, 'None')
train['WorkMLTeamSeatSelect'] = train['WorkMLTeamSeatSelect'].replace(np.nan, 'Other')
train['RemoteWork'] = train['RemoteWork'].replace(np.nan, 'Never')

test['WorkProductionFrequency'] = test['WorkProductionFrequency'].replace(np.nan, "Don't know")
test['LearningPlatformUsefulnessBlogs'] = test['LearningPlatformUsefulnessBlogs'].replace(np.nan, 'Not Useful')
test['LearningPlatformUsefulnessKaggle'] = test['LearningPlatformUsefulnessKaggle'].replace(np.nan, 'Not Useful')
test['LearningPlatformUsefulnessCourses'] = test['LearningPlatformUsefulnessCourses'].replace(np.nan, 'Not Useful')
test['LearningPlatformUsefulnessProjects'] = test['LearningPlatformUsefulnessProjects'].replace(np.nan, 'Not Useful')
test['LearningPlatformUsefulnessSO'] = test['LearningPlatformUsefulnessSO'].replace(np.nan, 'Not Useful')
test['LearningPlatformUsefulnessTextbook'] = test['LearningPlatformUsefulnessTextbook'].replace(np.nan, 'Not Useful')
test['LearningPlatformUsefulnessYouTube'] = test['LearningPlatformUsefulnessYouTube'].replace(np.nan, 'Not Useful')

test['WorkProductionFrequency'] = test['WorkProductionFrequency'].replace(np.nan, "Don't know")
test['WorkToolsFrequencyR'] = test['WorkToolsFrequencyR'].replace(np.nan, 'Rarely')
test['WorkToolsFrequencySQL'] = test['WorkToolsFrequencySQL'].replace(np.nan, 'Rarely')
test['WorkToolsFrequencyPython'] = test['WorkToolsFrequencyPython'].replace(np.nan, 'Rarely')
test['WorkMethodsFrequencyCross-Validation'] = test['WorkMethodsFrequencyCross-Validation'].replace(np.nan, 'Rarely')
test['WorkMethodsFrequencyDataVisualization'] = test['WorkMethodsFrequencyDataVisualization'].replace(np.nan, 'Rarely')
test['WorkMethodsFrequencyDecisionTrees'] = test['WorkMethodsFrequencyDecisionTrees'].replace(np.nan, 'Rarely')
test['WorkMethodsFrequencyLogisticRegression'] = test['WorkMethodsFrequencyLogisticRegression'].replace(np.nan, 'Rarely')

test['WorkMethodsFrequencyNeuralNetworks'] = test['WorkMethodsFrequencyNeuralNetworks'].replace(np.nan, 'Rarely')
test['WorkMethodsFrequencyPCA'] = test['WorkMethodsFrequencyPCA'].replace(np.nan, 'Rarely')
test['WorkMethodsFrequencyRandomForests'] = test['WorkMethodsFrequencyRandomForests'].replace(np.nan, 'Rarely')
test['WorkMethodsFrequencyTimeSeriesAnalysis'] = test['WorkMethodsFrequencyTimeSeriesAnalysis'].replace(np.nan, 'Rarely')
test['WorkChallengeFrequencyPolitics'] = test['WorkChallengeFrequencyPolitics'].replace(np.nan, 'Rarely')
test['WorkChallengeFrequencyUnusedResults'] = test['WorkChallengeFrequencyUnusedResults'].replace(np.nan, 'Rarely')
test['WorkChallengeFrequencyDirtyData'] = test['WorkChallengeFrequencyDirtyData'].replace(np.nan, 'Rarely')

test['WorkChallengeFrequencyExplaining'] = test['WorkChallengeFrequencyExplaining'].replace(np.nan, 'Rarely')
test['WorkChallengeFrequencyTalent'] = test['WorkChallengeFrequencyTalent'].replace(np.nan, 'Rarely')
test['WorkChallengeFrequencyClarity'] = test['WorkChallengeFrequencyClarity'].replace(np.nan, 'Rarely')
test['WorkChallengeFrequencyDataAccess'] = test['WorkChallengeFrequencyDataAccess'].replace(np.nan, 'Rarely')

test['DataScienceIdentitySelect'] = test['DataScienceIdentitySelect'].replace(np.nan, 'No')
test['TitleFit'] = test['TitleFit'].replace(np.nan, 'Poorly')
test['GenderSelect'] = test['GenderSelect'].replace(np.nan, 'A different identity')
test['Country'] = test['Country'].replace(np.nan, 'Other')
test['LanguageRecommendationSelect'] = test['LanguageRecommendationSelect'].replace(np.nan, 'Other')
test['FormalEducation'] = test['FormalEducation'].replace(np.nan, 'I prefer not to answer')
test['MajorSelect'] = test['MajorSelect'].replace(np.nan, 'Other')
test['Tenure'] = test['Tenure'].replace(np.nan, "I don't write code to analyze data")
test['EmployerIndustry'] = test['EmployerIndustry'].replace(np.nan, 'Other')
test['EmployerSize'] = test['EmployerSize'].replace(np.nan, 'I prefer not to answer')
test['WorkAlgorithmsSelect'] = test['WorkAlgorithmsSelect'].replace(np.nan, 'Other')
test['WorkDataVisualizations'] = test['WorkDataVisualizations'].replace(np.nan, 'None')
test['WorkMLTeamSeatSelect'] = test['WorkMLTeamSeatSelect'].replace(np.nan, 'Other')
test['RemoteWork'] = test['RemoteWork'].replace(np.nan, 'Never')

# Check # of null values for each column/feature
train.isnull().sum()
test.isnull().sum()

del train['CurrentEmployerType']
del test['CurrentEmployerType']

del train['EmploymentStatus']
del test['EmploymentStatus']

# Check # of null values for each column/feature
train.isnull().sum()
test.isnull().sum()

train['Tenure'].unique()

# Delete some columns/features having more than 1 null&NaN values
train = train.dropna(axis =1, thresh = 5529)
test = test.dropna(axis=1, thresh=1000)

test.info()

ordinal_columns = {'Rarely':1, 'Sometimes':2, 'Often':3, 'Most of the time': 4}

changing_columns = ['WorkToolsFrequencyPython','WorkToolsFrequencyR','WorkToolsFrequencySQL','WorkMethodsFrequencyCross-Validation', 'WorkMethodsFrequencyDataVisualization','WorkMethodsFrequencyDecisionTrees', 'WorkMethodsFrequencyLogisticRegression', 'WorkMethodsFrequencyNeuralNetworks', 'WorkMethodsFrequencyPCA', 'WorkMethodsFrequencyRandomForests', 'WorkMethodsFrequencyTimeSeriesAnalysis', 'WorkChallengeFrequencyPolitics', 'WorkChallengeFrequencyUnusedResults', 'WorkChallengeFrequencyDirtyData', 'WorkChallengeFrequencyExplaining', 'WorkChallengeFrequencyTalent', 'WorkChallengeFrequencyClarity', 'WorkChallengeFrequencyDataAccess']

# Transform non-numerical columns into numerical columns
from sklearn.preprocessing import OrdinalEncoder
enc = OrdinalEncoder()
for column in changing_columns:
  if train[column].dtype == np.float64:
    continue
  train[column] = train[column].replace(ordinal_columns)

# Transform non-numerical columns into numerical columns
from sklearn.preprocessing import OrdinalEncoder
enc = OrdinalEncoder()
for column in changing_columns:
  if test[column].dtype == np.float64:
    continue
  test[column] = test[column].replace(ordinal_columns)

# Transform non-numerical columns into numerical columns
from sklearn.preprocessing import LabelEncoder

for column in train.columns:
  if train[column].dtype == np.float64 or train[column].dtype == np.int64:
    continue
  train[column] = LabelEncoder().fit_transform(train[column].astype(str))

# Transform non-numerical columns into numerical columns
from sklearn.preprocessing import LabelEncoder

for column in test.columns:
  if test[column].dtype == np.float64 or test[column].dtype == np.int64:
    continue
  test[column] = LabelEncoder().fit_transform(test[column].astype(str))

train.head()

# Split 80-20

from sklearn.model_selection import train_test_split

train_x, valid_x, train_y, valid_y = train_test_split(train,train_label,test_size=0.2,random_state=0)
print("Train Data Shape: ", train_x.shape, "Train Label Shape: ", train_y.shape, "Validation Data Shape: ", valid_x.shape, "Validation Label Shape: ", valid_y.shape)

#x_train_deneme = pd.DataFrame(train_x)
#print(x_train_deneme.head())

from sklearn.ensemble import RandomForestRegressor
# Create and train a classifier 

clf = RandomForestRegressor(n_estimators=100, min_samples_split=30, max_depth=80, min_samples_leaf=5)
clf.fit(train_x, train_y)

prediction = clf.predict(valid_x)
from sklearn.metrics import mean_squared_error

rms = mean_squared_error(valid_y, prediction, squared=False)
print('Root Mean Squared Error for Validation : ', rms)

test_prediction = clf.predict(test)
test_prediction

pd.DataFrame(test_prediction).to_csv("submission12.csv")

train['WorkMethodsFrequencyNeuralNetworks'].unique()

train.head()

# Print the information about the dataset
print("Information about train data ",train.info())
print("Information about train data ",test.info())

train_new_new.info()

test_new.info()

train_new.info()

# Transform the categorical / ordinal attributes

#TitleFit_map = {"Poorly": 0, "Fine": 1, "Perfectly": 2}
#train_new['TitleFit'] = train_new['TitleFit'].replace(TitleFit_map)
#test_new['TitleFit'] = test_new['TitleFit'].replace(TitleFit_map)

FormalEducation_map = {"Bachelor's degree": 0, "Master's degree": 1, "Doctoral degree": 2,"Some college/university study without earning a bachelor's degree": 3, "Professional degree": 4, "I did not complete any formal education past high school": 5,"I prefer not to answer": 6}
train_new['FormalEducation'] = train_new['FormalEducation'].replace(FormalEducation_map)
test_new['FormalEducation'] = test_new['FormalEducation'].replace(FormalEducation_map)

#MajorSelect_map = {"Information technology, networking, or system administration": 0, "Computer Science": 1, "Mathematics or statistics": 2,"Engineering (non-computer focused)": 3, "Physics": 4, "Biology": 5,"Electrical Engineering": 6,"A social science": 7, "Other": 8, "Psychology": 9,"A health science": 10, "Fine arts or performing arts": 11, "A humanities discipline": 12,"Management information systems": 13, "I never declared a major": 14}
#train_new['MajorSelect'] = train_new['MajorSelect'].replace(MajorSelect_map)
#test_new['MajorSelect'] = test_new['MajorSelect'].replace(MajorSelect_map)

Tenure_map = {"3 to 5 years": 0, "1 to 2 years": 1, "6 to 10 years": 2,"More than 10 years": 3, "Less than a year": 4, "I don't write code to analyze data": 5}
train_new['Tenure'] = train_new['Tenure'].replace(Tenure_map)
test_new['Tenure'] = test_new['Tenure'].replace(Tenure_map)

WorkProductionFrequency_map = {"Always": 0, "Rarely": 1, "Sometimes": 2,"Most of the time": 3, "Never": 4, "Don't know": 5}
train_new['WorkProductionFrequency'] = train_new['WorkProductionFrequency'].replace(WorkProductionFrequency_map)
test_new['WorkProductionFrequency'] = test_new['WorkProductionFrequency'].replace(WorkProductionFrequency_map)

#WorkDataVisualizations_map = {"51-75% of projects": 0, "100% of projects": 1, "10-25% of projects": 2,"76-99% of projects": 3, "Less than 10% of projects": 4, "26-50% of projects": 5,"None": 6}
#train_new['WorkDataVisualizations'] = train_new['WorkDataVisualizations'].replace(WorkDataVisualizations_map)
#test_new['WorkDataVisualizations'] = test_new['WorkDataVisualizations'].replace(WorkDataVisualizations_map)

#WorkInternalVsExternalTools_map = {"Approximately half internal and half external": 0, "More internal than external": 1, "Do not know": 2,"Entirely internal": 3, "Entirely external": 4, "More external than internal": 5}
#train_new['WorkInternalVsExternalTools'] = train_new['WorkInternalVsExternalTools'].replace(WorkInternalVsExternalTools_map)
#test_new['WorkInternalVsExternalTools'] = test_new['WorkInternalVsExternalTools'].replace(WorkInternalVsExternalTools_map)

WorkMLTeamSeatSelect_map = {"Standalone Team": 0, "Business Department": 1, "Other": 2,"IT Department": 3, "Central Insights Team": 4}
train_new['WorkMLTeamSeatSelect'] = train_new['WorkMLTeamSeatSelect'].replace(WorkMLTeamSeatSelect_map)
test_new['WorkMLTeamSeatSelect'] = test_new['WorkMLTeamSeatSelect'].replace(WorkMLTeamSeatSelect_map)

#RemoteWork_map = {"Rarely": 0, "Most of the time": 1, "Never": 2,"Sometimes": 3, "Always": 4, "Don't know": 5}
#train_new['RemoteWork'] = train_new['RemoteWork'].replace(RemoteWork_map)
#test_new['RemoteWork'] = test_new['RemoteWork'].replace(RemoteWork_map)

train_new.info()

train_new['WorkMLTeamSeatSelect'].unique()

array = []
for colll in train_new.columns:
  if test_new[colll].dtype == np.float64:
    continue
  a = train_new[colll].unique()
  array.append(len(a))

train_new_append = train_new.append(test_new)

testtt_prediction = clf.predict(test_new)
pd.DataFrame(testtt_prediction).to_csv("submission7.csv")

train_new=train_new.drop('EmploymentStatus', axis = 1)
train_new=train_new.drop('Age', axis = 1)
#train_new=train_new.drop('GenderSelect', axis = 1)
train_new=train_new.drop('CurrentJobTitleSelect', axis = 1)
train_new=train_new.drop('CurrentEmployerType', axis = 1)
train_new=train_new.drop('MLToolNextYearSelect', axis = 1)
train_new=train_new.drop('MLMethodNextYearSelect', axis = 1)
train_new=train_new.drop('LanguageRecommendationSelect', axis = 1)
#train_new=train_new.drop('FormalEducation', axis = 1)
train_new=train_new.drop('MajorSelect', axis = 1)
train_new=train_new.drop('PastJobTitlesSelect', axis = 1)
train_new=train_new.drop('MLSkillsSelect', axis = 1)
train_new=train_new.drop('MLTechniquesSelect', axis = 1)
train_new=train_new.drop('WorkAlgorithmsSelect', axis = 1)
train_new=train_new.drop('WorkInternalVsExternalTools', axis = 1)
#train_new=train_new.drop('WorkMLTeamSeatSelect', axis = 1)
train_new=train_new.drop('EmployerIndustry', axis = 1)
train_new=train_new.drop('EmployerSize', axis = 1)
train_new=train_new.drop('TitleFit', axis = 1)
train_new=train_new.drop('RemoteWork', axis = 1)
train_new=train_new.drop('WorkDataVisualizations', axis = 1)



test_new=test_new.drop('WorkDataVisualizations', axis = 1)
test_new=test_new.drop('RemoteWork', axis = 1)
test_new=test_new.drop('TitleFit', axis = 1)
test_new=test_new.drop('EmploymentStatus', axis = 1)
test_new=test_new.drop('Age', axis = 1)
#test_new=test_new.drop('GenderSelect', axis = 1)
test_new=test_new.drop('CurrentJobTitleSelect', axis = 1)
test_new=test_new.drop('CurrentEmployerType', axis = 1)
test_new=test_new.drop('MLToolNextYearSelect', axis = 1)
test_new=test_new.drop('MLMethodNextYearSelect', axis = 1)
test_new=test_new.drop('LanguageRecommendationSelect', axis = 1)
#test_new=test_new.drop('FormalEducation', axis = 1)
test_new=test_new.drop('MajorSelect', axis = 1)
test_new=test_new.drop('PastJobTitlesSelect', axis = 1)
test_new=test_new.drop('MLSkillsSelect', axis = 1)
test_new=test_new.drop('MLTechniquesSelect', axis = 1)
test_new=test_new.drop('WorkAlgorithmsSelect', axis = 1)
test_new=test_new.drop('WorkInternalVsExternalTools', axis = 1)
#test_new=test_new.drop('WorkMLTeamSeatSelect', axis = 1)
test_new=test_new.drop('EmployerIndustry', axis = 1)
test_new=test_new.drop('EmployerSize', axis = 1)

from sklearn.svm import LinearSVC
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

clf = make_pipeline(StandardScaler(),LinearSVC(random_state=0, tol=1e-5))

cl = clf_logistic.predict(valid_x)
from sklearn.metrics import mean_squared_error

rms = mean_squared_error(valid_y, logis_pred, squared=False)
print('Root Mean Squared Error for Validation : ', rms)

logis_pred = clf_logistic.predict(valid_x)
from sklearn.metrics import mean_squared_error

rms = mean_squared_error(valid_y, logis_pred, squared=False)
print('Root Mean Squared Error for Validation : ', rms)

from sklearn.naive_bayes import GaussianNB

Gauss = GaussianNB()
Gauss.fit(train_x, train_y)

gauss_pred = Gauss.predict(valid_x)
from sklearn.metrics import mean_squared_error

rms = mean_squared_error(valid_y, gauss_pred, squared=False)
print('Root Mean Squared Error for Validation : ', rms)

from sklearn.linear_model import LogisticRegression

weights = {1:1.0, 2:2.0, 3:1.1, 4:4.0, 5:1.7, 6:1.8, 7:1.4, 8:1.4, 9:1.5, 10:2.3}

clf_logistic = LogisticRegression(solver = 'liblinear',class_weight=weights)
clf_logistic.fit(train_x,train_y)

from sklearn.svm import SVR
svm = SVR(kernel='linear', C=1)   # class weight???
clf= svm.fit(train_x, train_y)

svm_pred = clf.predict(valid_x)
from sklearn.metrics import mean_squared_error

rms = mean_squared_error(valid_y, svm_pred, squared=False)
print('Root Mean Squared Error for Validation : ', rms)

from sklearn.neural_network import MLPRegressor
mlp = MLPRegressor(hidden_layer_sizes=(400,200,100), activation='relu', solver='sgd',batch_size=512)
clf = mlp.fit(train_x, train_y)

mlp_pred = clf.predict(valid_x)
from sklearn.metrics import mean_squared_error

rms = mean_squared_error(valid_y, mlp_pred, squared=False)
print('Root Mean Squared Error for Validation : ', rms)

from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(test_label,predictions))
print(classification_report(test_label,predictions))

from sklearn.preprocessing import PolynomialFeatures

transformer = PolynomialFeatures(degree=2, include_bias=False)
transformer.fit(train_x)
x_ = transformer.transform(train_x)
transformer.fit(valid_x)
valid_x_ = transformer.transform(valid_x)

from sklearn.linear_model import LinearRegression
model = LinearRegression().fit(x_, train_y)
y_pred2 = model.predict(valid_x_)

from sklearn.metrics import mean_squared_error

rms = mean_squared_error(valid_y, y_pred2, squared=False)
print('Root Mean Squared Error for Validation : ', rms)

y_pred2

from sklearn.linear_model import LinearRegression

model = LinearRegression().fit(train_x, train_y)
r_sq = model.score(train_x, train_y)

y_pred = model.predict(valid_x)
from sklearn.metrics import mean_squared_error

rms = mean_squared_error(valid_y, y_pred, squared=False)
print('Root Mean Squared Error for Validation : ', rms)

from keras.models import Sequential
from keras.layers import Dense

# define base model
def baseline_model():
    model = Sequential()
    model.add(Dense(60, input_dim=10, kernel_initializer='normal',activation='relu'))
    model.add(Dense(20, activation='relu'))
    model.add(Dense(10, activation='relu'))
    model.add(Dense(1, activation='linear'))
    model.compile(loss='mean_squared_error', optimizer='adam')
    return model

from keras.wrappers.scikit_learn import KerasRegressor
from sklearn.model_selection import cross_val_score

estimator = KerasRegressor(build_fn=baseline_model, epochs=50, batch_size=32, verbose=1)

estimator.fit(train_x,train_y)

pred = estimator.predict(valid_x)
from sklearn.metrics import mean_squared_error

rms = mean_squared_error(valid_y, pred, squared=False)
print('Root Mean Squared Error for Validation : ', rms)

pred

test_prediction = clf.predict(test_new)
test_prediction

pd.DataFrame(test_prediction).to_csv("submission4.csv")

# What are the 10 most important features?

importances = clf.feature_importances_
std = np.std([tree.feature_importances_ for tree in clf.estimators_], axis=0)
indices = np.argsort(importances)[::-1]

# Print the feature ranking
print("Feature ranking:")

for f in range(45):
    print("%d. %s (%f)" % (f + 1, train_x.columns[indices[f]-1], importances[indices[f]]))

# Plot the feature importances
import matplotlib.pyplot as plt
plt.figure()
plt.title("Feature importances")
plt.bar(range(45), importances[indices[:45]], color="r", yerr=std[indices[:45]], align="center")
plt.xticks(range(45), train_x.columns[indices[:45]-1], rotation='vertical')
plt.show()

importance = clf.feature_importances_
# summarize feature importance
for i,v in enumerate(importance):
	print('Feature: %0d, Score: %.5f' % (i,v))
# plot feature importance
plt.bar([x for x in range(len(importance))], importance)
plt.show()

train_new.columns

pd.DataFrame(valid_y).to_csv("submission3.csv")